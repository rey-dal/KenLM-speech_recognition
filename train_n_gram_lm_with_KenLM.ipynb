{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AJ9UTy4hQzL"
      },
      "source": [
        "# Train n-gram language model with KenLM on Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtkgQE7--Ufg"
      },
      "source": [
        "https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Boosting_Wav2Vec2_with_n_grams_in_Transformers.ipynb#scrollTo=X9qg4FPt2zi8 for detailed explanation on how to use KenLM to boost wav2vec2 fine-tuned models on Huggingface ü§ó"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBCqCboC6Soc"
      },
      "source": [
        "Install KenLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CKLr9bI6GPE",
        "outputId": "31b2404e-9d99-4562-bdc0-6787ecf87200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libboost-program-options-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "libboost-system-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "libboost-thread-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "libbz2-dev is already the newest version (1.0.8-5build1).\n",
            "liblzma-dev is already the newest version (5.2.5-2ubuntu1).\n",
            "libboost-test-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "libeigen3-dev is already the newest version (3.4.0-2ubuntu2).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu9.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TIlrFi3M6XO4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff31836b-1613-4ae2-c734-4aaa2042db3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-26 14:02:15--  https://kheafield.com/code/kenlm.tar.gz\n",
            "Resolving kheafield.com (kheafield.com)... 129.80.89.152, 2603:c020:4009:8710:ca:11:17:0\n",
            "Connecting to kheafield.com (kheafield.com)|129.80.89.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 491888 (480K) [application/octet-stream]\n",
            "Saving to: ‚ÄòSTDOUT‚Äô\n",
            "\n",
            "-                   100%[===================>] 480.36K  1.93MB/s    in 0.2s    \n",
            "\n",
            "2024-11-26 14:02:16 (1.93 MB/s) - written to stdout [491888/491888]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGwSg6Bl6a8Y",
        "outputId": "d4b87eec-ad85-4145-847e-432bc5c869be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‚Äòkenlm/build‚Äô: File exists\n",
            "build_binary  fragment\t       lmplz\t\t\t     query\n",
            "count_ngrams  interpolate      phrase_table_vocab\t     streaming_example\n",
            "filter\t      kenlm_benchmark  probing_hash_table_benchmark\n"
          ]
        }
      ],
      "source": [
        "!mkdir kenlm/build && cd kenlm/build && cmake .. && make -j2\n",
        "!ls kenlm/build/bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUUGXbDy6x7r"
      },
      "source": [
        "Install ü§ó dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs8LAZKr6wF8",
        "outputId": "6be64d63-5072-45fb-ad3c-a9260093a3a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RoHBmOz66fz"
      },
      "source": [
        "Load preprocessed dataset from ü§ó and write it to file as required by KenLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bDpNg9c6mUu",
        "outputId": "684cb4d2-a844-44a0-c9f2-60c507a3c028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# change to your dataset path\n",
        "username = \"hf-test\"\n",
        "target_lang = \"sv\"\n",
        "\n",
        "dataset = load_dataset(f\"{username}/{target_lang}_corpora_parliament_processed\", split=\"train\")\n",
        "\n",
        "with open(\"text.txt\", \"w\") as file:\n",
        "  file.write(\" \".join(dataset[\"text\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8PqeGC17jD8"
      },
      "source": [
        "Train 5-gram language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8KoINuj7h-1",
        "outputId": "92222d32-0618-4b6d-daa2-ac272c2da36c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/text.txt\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Unigram tokens 42153890 types 360209\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:4322508 2:1061777088 3:1990832128 4:3185331200 5:4645275136\n",
            "Statistics:\n",
            "1 360208 D1=0.686222 D2=1.01595 D3+=1.33685\n",
            "2 5476741 D1=0.761523 D2=1.06735 D3+=1.32559\n",
            "3 18177681 D1=0.839918 D2=1.12061 D3+=1.33794\n",
            "4 30374983 D1=0.909146 D2=1.20496 D3+=1.37235\n",
            "5 37231651 D1=0.944104 D2=1.25164 D3+=1.344\n",
            "Memory estimate for binary LM:\n",
            "type      MB\n",
            "probing 1884 assuming -p 1.5\n",
            "probing 2195 assuming -r models -p 1.5\n",
            "trie     922 without quantization\n",
            "trie     518 assuming -q 8 -b 8 quantization \n",
            "trie     806 assuming -a 22 array pointer compression\n",
            "trie     401 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:4322496 2:87627856 3:363553620 4:728999592 5:1042486228\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:4322496 2:87627856 3:363553620 4:728999592 5:1042486228\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:10804240 kB\tVmRSS:14068 kB\tRSSMax:2918368 kB\tuser:109.162\tsys:29.7917\tCPU:138.954\treal:222.01\n"
          ]
        }
      ],
      "source": [
        "!kenlm/build/bin/lmplz -o 5 <\"text.txt\" > \"5gram.arpa\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ5OKh358nwR"
      },
      "source": [
        "Check head of file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv93ZCR68s4m",
        "outputId": "ff68a263-89d5-4fc8-d27f-ceafddce650e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\data\\\n",
            "ngram 1=360208\n",
            "ngram 2=5476741\n",
            "ngram 3=18177681\n",
            "ngram 4=30374983\n",
            "ngram 5=37231651\n",
            "\n",
            "\\1-grams:\n",
            "-6.770219\t<unk>\t0\n",
            "0\t<s>\t-0.11831701\n",
            "-4.6095004\t√•terupptagande\t-1.2174699\n",
            "-2.2361007\tav\t-0.79668784\n",
            "-4.8163533\tsessionen\t-0.37327805\n",
            "-2.2251768\tjag\t-1.4205662\n",
            "-4.181505\tf√∂rklarar\t-0.56261665\n",
            "-3.5790775\teuropaparlamentets\t-0.63611007\n",
            "-4.771945\tsession\t-0.3647111\n",
            "-5.8043895\t√•terupptagen\t-0.3058712\n",
            "-2.8580177\tefter\t-0.7557702\n",
            "-5.199537\tavbrottet\t-0.43322718\n"
          ]
        }
      ],
      "source": [
        "!head -20 5gram.arpa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEcPijF77mPY"
      },
      "source": [
        "Add end-of-sentence token \"\\</s>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Sktd-U5a7yZL"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/wikipedia_100M.fr\", \"r\") as read_file, open(\"/content/wikipedia.fr\", \"w\") as write_file:\n",
        "  has_added_eos = False\n",
        "  for line in read_file:\n",
        "    if not has_added_eos and \"ngram 1=\" in line:\n",
        "      count=line.strip().split(\"=\")[-1]\n",
        "      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
        "    elif not has_added_eos and \"<s>\" in line:\n",
        "      write_file.write(line)\n",
        "      write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
        "      has_added_eos = True\n",
        "    else:\n",
        "      write_file.write(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqXHYY-K760Q"
      },
      "source": [
        "Check head of file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "0QuHk3AY8Hax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb11d5f7-5ba7-4087-947c-b8438db06536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\data\\\n",
            "ngram 1=360209\n",
            "ngram 2=5476741\n",
            "ngram 3=18177681\n",
            "ngram 4=30374983\n",
            "ngram 5=37231651\n",
            "\n",
            "\\1-grams:\n",
            "-6.770219\t<unk>\t0\n",
            "0\t<s>\t-0.11831701\n",
            "0\t</s>\t-0.11831701\n",
            "-4.6095004\t√•terupptagande\t-1.2174699\n",
            "-2.2361007\tav\t-0.79668784\n",
            "-4.8163533\tsessionen\t-0.37327805\n",
            "-2.2251768\tjag\t-1.4205662\n",
            "-4.181505\tf√∂rklarar\t-0.56261665\n",
            "-3.5790775\teuropaparlamentets\t-0.63611007\n",
            "-4.771945\tsession\t-0.3647111\n",
            "-5.8043895\t√•terupptagen\t-0.3058712\n",
            "-2.8580177\tefter\t-0.7557702\n"
          ]
        }
      ],
      "source": [
        "!head -20 5gram_sv_lm.arpa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTvRntrZ9-uq"
      },
      "source": [
        "Compress arpa file by converting it to bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "DnmOlNZ5-ClT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d21462de-4be4-4c3e-f276-8b3a3b44b5c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading 5gram_sv_lm.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n"
          ]
        }
      ],
      "source": [
        "!kenlm/build/bin/build_binary 5gram_sv_lm.arpa 5gram_sv_lm.bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xra-pM-M8MZj"
      },
      "source": [
        "Download file to local machine (use Chrome if it fails on another browser)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"5gram_sv_lm.bin\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "CZ9pgChgptat",
        "outputId": "e8520ef3-8667-4814-84c8-3edd92f09456"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5e185f5c-9acd-4f7a-aeb8-d2a9ae4c5b06\", \"5gram_sv_lm.bin\", 1981380707)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kenlm/build/bin/lmplz -o 3 < wikipedia.fr > wikipedia.arpa\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6ThVxWA2ZVZ",
        "outputId": "1bb69a13-0643-4146-c4b1-702833d6747e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/wikipedia.fr\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Unigram tokens 17190965 types 312291\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:3747492 2:3785666560 3:7098124800\n",
            "Statistics:\n",
            "1 312291 D1=0.678269 D2=1.00641 D3+=1.35207\n",
            "2 3320716 D1=0.770153 D2=1.07759 D3+=1.35281\n",
            "3 8731639 D1=0.828302 D2=1.15594 D3+=1.33959\n",
            "Memory estimate for binary LM:\n",
            "type     MB\n",
            "probing 233 assuming -p 1.5\n",
            "probing 253 assuming -r models -p 1.5\n",
            "trie    101 without quantization\n",
            "trie     58 assuming -q 8 -b 8 quantization \n",
            "trie     94 assuming -a 22 array pointer compression\n",
            "trie     52 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:3747492 2:53131456 3:174632780\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:3747492 2:53131456 3:174632780\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:10780640 kB\tVmRSS:12084 kB\tRSSMax:2647004 kB\tuser:17.5156\tsys:5.16534\tCPU:22.681\treal:26.3517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kenlm/build//bin/build_binary wikipedia.arpa wikipedia.binary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scdzRHm02wZS",
        "outputId": "26286719-5c29-4073-8623-491ebfe567cd"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading wikipedia.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kenlm/build//bin/query wikipedia.binary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5dGxgjm22Xh",
        "outputId": "4649041a-7301-4370-f588-d6f121440114"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This binary file contains probing hash tables.\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kenlm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "73EHVTcW3I00",
        "outputId": "a557853a-d66b-4d47-f205-d9837e5985d1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kenlm\n",
            "  Downloading kenlm-0.2.0.tar.gz (427 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/427.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m \u001b[32m419.8/427.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m427.4/427.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.2.0-cp310-cp310-linux_x86_64.whl size=3184463 sha256=462442406e553c41a277a0a3e8fd84a39363b6dd21e5524420f66d23b527d6c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/80/e0/18f4148e863fb137bd87e21ee2bf423b81b3ed6989dab95135\n",
            "Successfully built kenlm\n",
            "Installing collected packages: kenlm\n",
            "Successfully installed kenlm-0.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "kenlm"
                ]
              },
              "id": "7c4f8d56cb5441bfb0319f30562f311f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kenlm\n",
        "# Charge le mod√®le binaire\n",
        "model = kenlm.LanguageModel('wikipedia.binary')\n",
        "# Test la probabilit√© d'une phrase\n",
        "sentence = \"bonjour comment allezvous\"\n",
        "print(f\"Score de la phrase : {model.score(sentence)}\") #log probabilit√©\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYxck9nv2848",
        "outputId": "308cadf5-022d-486b-a9f8-754b9815923f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score de la phrase : -15.604488372802734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"Ceci est une phrase test.\"\n",
        "perplexity = model.perplexity(test_sentence)\n",
        "print(f\"Perplexit√© : {perplexity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNC5JbTL3SPd",
        "outputId": "be13ff62-0d86-4544-9463-7e6e9d42e820"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexit√© : 6399.035428578444\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}